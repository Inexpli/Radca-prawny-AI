import os
import glob
import json
import textwrap
import uuid
import streamlit as st
from datetime import datetime
from typing import List, Dict, Tuple


st.set_page_config(
    page_title="Radca Prawny AI",
    page_icon="‚öñÔ∏è",
    layout="wide"
)

st.title("‚öñÔ∏è Radca Prawny AI")
st.markdown("Tw√≥j prywatny asystent prawny.")

loading_placeholder = st.empty()
loading_placeholder.info("üöÄ Inicjalizacja systemu... \n\n üõ†Ô∏è ≈Åadowanie bibliotek AI (to mo≈ºe chwilƒô potrwaƒá)...")

CONFIG = {
    "SEARCHING_COLLECTION": "polskie_prawo",
    "SESSIONS_DIR": "sessions",
    "MODEL_ID": "speakleash/Bielik-11B-v2.6-Instruct",
    "QDRANT_PATH": "./qdrant_data",
    "DENSE_MODEL": "intfloat/multilingual-e5-large",
    "SPARSE_MODEL": "Qdrant/bm25",
    "RERANKER_MODEL": "sdadas/polish-reranker-roberta-v3",

    "RAG": {
        "TOP_K": 10,
        "FETCH_K": 50,
    },

    "NAME_SESSION_CONFIG": {
        "max_new_tokens": 32,
        "temperature": 0.3,
    },

    "REWRITING_CONFIG": {
        "max_new_tokens": 128,
        "temperature": 0.1,
    },

    "GENERATING_CONFIG": {
        "max_new_tokens": 1500,
        "temperature": 0.1,
        "repetition_penalty": 1.05,
    },

    "NAMING_SESSION_PROMPT": 
    textwrap.dedent("""
        Jeste≈õ asystentem, kt√≥ry tworzy zwiƒôz≈Çe tytu≈Çy dla rozm√≥w na podstawie pierwszego pytania u≈ºytkownika.
        ZASADY:
        1. Tytu≈Ç musi byƒá kr√≥tki (maksymalnie 5 s≈Ç√≥w).
        2. Tytu≈Ç musi byƒá precyzyjny i odzwierciedlaƒá temat pytania.
        3. Unikaj og√≥lnych fraz jak "Rozmowa z AI" czy "Pytanie prawne".
        4. U≈ºywaj jƒôzyka polskiego.
        5. Wypisz tytu≈Ç w formie, kt√≥rƒÖ mogƒô wpisaƒá w Google.
        PIERWSZE PYTANIE: "{question}"
        TYTU≈Å ROZMOWY:
    """).strip(),

    "REWRITING_PROMPT": 
    textwrap.dedent("""
        Jeste≈õ prawnikiem-lingwistƒÖ. Twoim zadaniem jest przet≈Çumaczenie potocznego pytania klienta na profesjonalne zapytanie do wyszukiwarki prawniczej.
        ZASADY:
        1. Zamie≈Ñ s≈Çowa potoczne na ustawowe (np. "morderstwo" -> "zab√≥jstwo", "ukrad≈Ç auto" -> "zab√≥r pojazdu mechanicznego").
        2. Uwzglƒôdnij kontekst z historii rozmowy (je≈õli jest).
        3. Wynik ma byƒá jednym, precyzyjnym zdaniem pytajƒÖcym.

        HISTORIA: {short_history}
        OSTATNIE PYTANIE: "{user_query}"

        PROFESJONALNE ZAPYTANIE:
    """).strip(),

    "SYSTEM_PROMPT": 
    textwrap.dedent("""
        Jeste≈õ ≈öCIS≈ÅYM analitykiem tekst√≥w prawnych. Twoim zadaniem jest przetworzenie DOSTARCZONEGO KONTEKSTU na odpowied≈∫.
                
        KRYTYCZNA ZASADA BEZPIECZE≈ÉSTWA (GROUNDING):
        1. Twoja wiedza ogranicza siƒô WY≈ÅƒÑCZNIE do tre≈õci podanej poni≈ºej w sekcji "KONTEKST PRAWNY".
        2. ZABRANIA SIƒò korzystania z wiedzy w≈Çasnej/treningowej modelu. Je≈õli przepisu nie ma w tek≈õcie - NIE ISTNIEJE.
        3. Je≈õli pytanie wykracza poza za≈ÇƒÖczony tekst, napisz: "Dostarczony materia≈Ç nie zawiera informacji na ten temat".
        4. Nie wymy≈õlaj artyku≈Ç√≥w, nie cytuj z pamiƒôci.
        5. Struktura odpowiedzi:
        - Podstawa Prawna (wymie≈Ñ artyku≈Çy i nazwy akt√≥w)
        - Analiza (interpretacja sytuacji w ≈õwietle przepis√≥w)
        - Konkluzja (jasne wnioski dla klienta)
        - Podsumowanie (zwiƒôz≈Çe streszczenie dla klienta)

        RESTKRYKCYJNE ZASADY FORMATOWANIA (MODEL MUSI ICH PRZESTRZEGAƒÜ):
        Ka≈ºda odpowied≈∫ musi sk≈Çadaƒá siƒô wy≈ÇƒÖcznie z 4 sekcji oznaczonych nag≈Ç√≥wkami H2 (##). Nie dodawaj ≈ºadnego tekstu przed pierwszƒÖ sekcjƒÖ ani po ostatniej.

        STRUKTURA ODPOWIEDZI:

        ## Podstawa Prawna
        W tej sekcji wymie≈Ñ przepisy w formie listy wypunktowanej.
        BEZWZGLƒòDNY FORMAT CYTOWANIA:
        * **Art. {{numer}} ¬ß {{numer_paragrafu}} {{Pe≈Çna Nazwa Kodeksu}}:** {{tre≈õƒá przepisu}}

        Zasady dla cytat√≥w:
        - Je≈õli przepis nie ma paragrafu, pomi≈Ñ znak ¬ß i numer paragrafu (np. Art. 148 Kodeksu Karnego:).
        - Zawsze podawaj pe≈ÇnƒÖ nazwƒô kodeksu (np. "Kodeksu Karnego", a nie "k.k.").
        - Tre≈õƒá przepisu musi byƒá przytoczona po dwukropku.

        ## Analiza
        Szczeg√≥≈Çowa interpretacja sytuacji w ≈õwietle przytoczonych wy≈ºej przepis√≥w. Odnie≈õ siƒô bezpo≈õrednio do fakt√≥w z zapytania u≈ºytkownika. Wyja≈õnij przes≈Çanki (np. "u≈ºycie przemocy", "stan nietrze≈∫wo≈õci"). Pisz akapitami.

        ## Konkluzja
        Jasne i zwiƒôz≈Çe wnioski. Je≈õli wynik zale≈ºy od zmiennych (np. czy u≈ºyto broni), zastosuj listƒô wypunktowanƒÖ, aby pokazaƒá warianty:
        * Wariant A: konsekwencja.
        * Wariant B: konsekwencja.

        ## Podsumowanie
        Jedno lub dwa zdania streszczenia dla klienta, stanowiƒÖce "tl;dr" ca≈Çej porady. Najlepiej by by≈Ço, gdyby zawiera≈Ço bezpo≈õredniƒÖ, konkretnƒÖ odpowied≈∫ na pytanie u≈ºytkownika.

        Na ko≈Ñcu odpowiedzi do≈ÇƒÖcz sekcjƒô ≈πr√≥d≈Ça, gdzie w jednej linii wymienisz wszystkie cytowane artyku≈Çy w formacie:
        BEZWZGLƒòDNY FORMAT WYPISYWANIA ≈πR√ìDE≈Å:
        "\n\n---\nüìö **≈πr√≥d≈Ça:** Art. {{numer}} {{Pe≈Çna Nazwa Kodeksu}}."
        Przyk≈Çad:
        "\n\n---\nüìö **≈πr√≥d≈Ça:** Art. 134, 135, 136, 148 Kodeksu Karnego."
        Nie zapisuj tego jako osobny nag≈Çowek, tylko jako zwyk≈Çy tekst od nowej linii oraz nie wypisuj paragraf√≥w w ≈∫r√≥d≈Çach.
    """).strip(),
}

if not os.path.exists(CONFIG["SESSIONS_DIR"]):
    os.makedirs(CONFIG["SESSIONS_DIR"])


@st.cache_resource
def load_resources() -> Tuple:
    """≈Åaduje zasoby AI: Qdrant, embeddery i model jƒôzykowy."""
    print("LOG: Importowanie bibliotek...")

    from unsloth import FastLanguageModel
    from qdrant_client import QdrantClient
    from fastembed import SparseTextEmbedding
    from sentence_transformers import SentenceTransformer, CrossEncoder


    client = QdrantClient(path=CONFIG["QDRANT_PATH"])
    dense = SentenceTransformer(CONFIG["EMBEDDING_MODEL"], device="cuda")
    sparse = SparseTextEmbedding(CONFIG["SPARSE_MODEL"], device="cuda")
    reranker = CrossEncoder(CONFIG["RERANKER_MODEL"], device="cuda")

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = CONFIG["MODEL_ID"],
        max_seq_length = 8192,
        dtype = None,
        load_in_4bit = True,
    )
    FastLanguageModel.for_inference(model)

    return client, dense, sparse, reranker, model, tokenizer

try:
    client, dense_embedder, sparse_embedder, reranker, model, tokenizer = load_resources()

    loading_placeholder.empty()
    st.toast("System gotowy do pracy!", icon="‚úÖ")
except Exception as e:
    st.error(f"B≈ÇƒÖd krytyczny podczas ≈Çadowania: {e}")
    st.stop()

import torch
from qdrant_client import models


def get_session_file_path(session_id):
    """Zwraca ≈õcie≈ºkƒô do pliku sesji na podstawie ID sesji."""
    return os.path.join(CONFIG["SESSIONS_DIR"], f"{session_id}.json")

def save_current_session():
    """Zapisuje bie≈ºƒÖcƒÖ sesjƒô do pliku JSON."""
    if not st.session_state.messages:
        return
    
    if "title" not in st.session_state:
        user_msgs = [m['content'] for m in st.session_state.messages if m['role'] == 'user']
        if user_msgs:
            question = user_msgs[0]
            st.session_state.title = name_session(question)

    data = {
        "id": st.session_state.session_id,
        "title": st.session_state.get("title", "Bez tytu≈Çu"),
        "timestamp": datetime.now().isoformat(),
        "messages": st.session_state.messages
    }
    
    file_path = get_session_file_path(st.session_state.session_id)
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def delete_session_by_id(session_id):
    """Usuwa plik sesji o podanym ID."""
    file_path = get_session_file_path(session_id)
    if os.path.exists(file_path):
        os.remove(file_path)

def load_session_by_id(session_id):
    """Wczytuje sesjƒô z pliku."""
    file_path = get_session_file_path(session_id)
    if os.path.exists(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            st.session_state.session_id = data["id"]
            st.session_state.messages = data["messages"]
            st.session_state.title = data.get("title", "Bez tytu≈Çu")
    else:
        init_new_session()

def init_new_session():
    """Resetuje stan do nowej, czystej rozmowy."""
    st.session_state.session_id = str(uuid.uuid4())
    st.session_state.messages = []
    if "title" in st.session_state:
        del st.session_state.title

def list_past_sessions():
    """Zwraca listƒô dostƒôpnych plik√≥w sesji posortowanƒÖ od najnowszej."""
    files = glob.glob(os.path.join(CONFIG["SESSIONS_DIR"], "*.json"))
    sessions = []
    for f in files:
        try:
            with open(f, "r", encoding="utf-8") as file:
                data = json.load(file)
                sessions.append({
                    "id": data["id"],
                    "title": data.get("title", "Bez tytu≈Çu"),
                    "path": f,
                    "time": os.path.getmtime(f)
                })
        except:
            continue
    return sorted(sessions, key=lambda x: x["time"], reverse=True)

if "session_id" not in st.session_state:
    init_new_session()

def name_session(question: str) -> str:
    """Nazywa sesjƒô na podstawie pierwszego pytania u≈ºytkownika."""
    prompt = CONFIG["NAMING_SESSION_PROMPT"].format(question=question)

    messages = [{"role": "user", "content": prompt}]
    inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs_tensor = tokenizer(inputs, return_tensors="pt").to("cuda")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs_tensor, 
            max_new_tokens=CONFIG["NAME_SESSION_CONFIG"]["max_new_tokens"],
            temperature=CONFIG["NAME_SESSION_CONFIG"]["temperature"],  
            do_sample=True,  
            use_cache=True
        )
    
    title = tokenizer.decode(outputs[0][inputs_tensor.input_ids.shape[1]:], skip_special_tokens=True).strip()
    cleaned_title = title.replace('"', '').strip()

    if len(cleaned_title) < 3: 
        return "Nowa rozmowa"

    return cleaned_title

def rewrite_query(user_query, chat_history) -> str:
    """
    Inteligentnie przepisuje kr√≥tkie pytania na pe≈Çne 
    zapytania do bazy, wykorzystujƒÖc historiƒô rozmowy.
    """
    if not chat_history:
        return user_query
    
    short_history = chat_history[-4:] 
    
    rewrite_prompt = CONFIG["REWRITING_PROMPT"].format(
        short_history="\n".join([f"{m['role']}: {m['content']}" for m in short_history]),
        user_query=user_query
    )

    messages = [{"role": "user", "content": rewrite_prompt}]
    inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs_tensor = tokenizer(inputs, return_tensors="pt").to("cuda")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs_tensor, 
            max_new_tokens=CONFIG["REWRITING_CONFIG"]["max_new_tokens"],
            temperature=CONFIG["REWRITING_CONFIG"]["temperature"],  
            do_sample=True,  
            use_cache=True
        )
    
    rewritten = tokenizer.decode(outputs[0][inputs_tensor.input_ids.shape[1]:], skip_special_tokens=True).strip()
    cleaned = rewritten.replace('"', '').replace("PROFESJONALNE ZAPYTANIE:", "").strip()

    if len(cleaned) < 3: 
        return user_query

    return cleaned

def search_law(query: str, top_k: int = CONFIG["RAG"]["TOP_K"], fetch_k: int = CONFIG["RAG"]["FETCH_K"]) -> List[Dict]:
    """
    Szuka w ka≈ºdej kolekcji, ≈ÇƒÖczy wyniki i zwraca X najlepszych globalnie.
    """
    dense_vec = dense_embedder.encode([f"query: {query}"], normalize_embeddings=True)[0].tolist()
    sparse_res = list(sparse_embedder.embed([query]))[0]
    
    qdrant_sparse = models.SparseVector(
        indices=sparse_res.indices.tolist(), 
        values=sparse_res.values.tolist()
    )

    initial_hits = []

    if client.collection_exists(CONFIG["RAG"]["SEARCHING_COLLECTION"]):
        hits = client.query_points(
            collection_name=CONFIG["RAG"]["SEARCHING_COLLECTION"],
            prefetch=[
                models.Prefetch(
                    query=dense_vec,
                    using="dense",
                    limit=fetch_k,
                ),
                models.Prefetch(
                    query=qdrant_sparse,
                    using="sparse",
                    limit=fetch_k,
                )
            ],
            query=models.FusionQuery(fusion=models.Fusion.RRF),
            limit=fetch_k
        ).points
        initial_hits = hits
        
        cross_inp = [[query, hit.payload.get('text', '')] for hit in initial_hits]
        cross_scores = reranker.predict(cross_inp)

        for idx, hit in enumerate(initial_hits):
            hit.score = cross_scores[idx]
        
        reranked_hits = sorted(initial_hits, key=lambda x: x.score, reverse=True)
        
        final_hits = reranked_hits[:top_k]

        return final_hits
    

with st.sidebar:
    if st.button("Nowy czat", use_container_width=True, type="secondary"):
        save_current_session()
        init_new_session()
        st.rerun()

    st.title("Historia")
    
    sessions = list_past_sessions()
    
    def handle_delete(session_id):
        """Obs≈Çuguje usuwanie sesji."""
        delete_session_by_id(session_id)
        if st.session_state.session_id == session_id:
            init_new_session()
        st.toast(f"Usuniƒôto rozmowƒô.", icon="üóëÔ∏è")

    st.caption("Poprzednie rozmowy:")

    st.markdown(
        """
        <style>
            div[data-testid="column"] {
                display: flex;
                align-items: center; 
            }
            div[data-testid="stVerticalBlock"] > div > div[data-testid="stHorizontalBlock"] {
                gap: 0.5rem;
            }
        </style>
        """, unsafe_allow_html=True
    )

    for s in sessions:
        is_active = s["id"] == st.session_state.session_id
        
        col1, col2 = st.columns([0.85, 0.15])
        
        with col1:
            display_title = s["title"] if len(s["title"]) < 28 else s["title"][:25] + "..."
            
            btn_type = "primary" if is_active else "secondary"
            
            if st.button(
                display_title, 
                key=f"load_{s['id']}", 
                use_container_width=True, 
                type=btn_type,
                help=s["title"]
            ):
                if not is_active:
                    save_current_session()
                    load_session_by_id(s["id"])
                    st.rerun()
        
        with col2:
            if st.button("‚úï", key=f"del_{s['id']}", type="tertiary", help="Usu≈Ñ trwale tƒô rozmowƒô"):
                handle_delete(s["id"])
                st.rerun()

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("O co chcesz zapytaƒá?"):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    save_current_session()

    context_text = ""

    with st.status("Analizujƒô przepisy...", expanded=True) as status:
        st.write("üîÑ Analizujƒô pytanie...")
        search_query = rewrite_query(prompt, st.session_state.messages[:-1])
        
        st.write("üîç Przeszukujƒô Kodeksy...")
        hits = search_law(search_query, top_k=5)
        
        if hits:
            for hit in hits:
                meta = hit.payload
                source_label = meta.get('source', 'Akt Prawny')
                article_label = meta.get('article', 'Art. ?')
                text_content = meta.get('full_markdown', meta.get('text', ''))
                
                context_text += f"=== {source_label} | {article_label} ===\n{text_content}\n\n"
        else:
            context_text = "Brak przepis√≥w."
            
        status.update(label="Analiza zako≈Ñczona!", state="complete", expanded=False)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()

        if not hits or context_text.strip() == "Brak przepis√≥w.":
            response = """
            **Brak danych w bazie.** \n\nNie znalaz≈Çem w dostƒôpnych kodeksach przepis√≥w pasujƒÖcych precyzyjnie do Twojego zapytania. Jako system RAG nie mogƒô generowaƒá porad prawnych z pamiƒôci, aby uniknƒÖƒá wprowadzenia Ciƒô w b≈ÇƒÖd nieaktualnymi przepisami.
            """
            message_placeholder.markdown(response)
        
        with st.spinner("Piszƒô opiniƒô prawnƒÖ..."):
            system_prompt = CONFIG["SYSTEM_PROMPT"]
            
            messages_payload = [{"role": "system", "content": system_prompt}]
            for msg in st.session_state.messages[:-1]:
                messages_payload.append(msg)
                
            current_input = f"KONTEKST PRAWNY:\n{context_text}\n\nPYTANIE:\n{prompt}"
            messages_payload.append({"role": "user", "content": current_input})

            model_inputs = tokenizer.apply_chat_template(messages_payload, tokenize=False, add_generation_prompt=True)
            inputs_tensor = tokenizer(model_inputs, return_tensors="pt").to("cuda")

            with torch.no_grad():
                outputs = model.generate(
                    **inputs_tensor,
                    max_new_tokens=CONFIG["GENERATING_CONFIG"]["max_new_tokens"],
                    temperature=CONFIG["GENERATING_CONFIG"]["temperature"],
                    repetition_penalty=CONFIG["GENERATING_CONFIG"]["repetition_penalty"],
                    do_sample=True,
                    use_cache=True,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0][inputs_tensor.input_ids.shape[1]:], skip_special_tokens=True)

        message_placeholder.markdown(response)

    st.session_state.messages.append({"role": "assistant", "content": response})
    save_current_session()
    if len(st.session_state.messages) == 2:
        st.rerun()