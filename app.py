import os
import glob
import json
import uuid
import streamlit as st
from datetime import datetime
from typing import List, Dict, Tuple


st.set_page_config(
    page_title="Radca Prawny AI",
    page_icon="âš–ï¸",
    layout="wide"
)

st.title("âš–ï¸ Radca Prawny AI")
st.markdown("TwÃ³j prywatny asystent prawny.")

loading_placeholder = st.empty()
loading_placeholder.info("ğŸš€ Inicjalizacja systemu... \n\n ğŸ› ï¸ Åadowanie bibliotek AI (to moÅ¼e chwilÄ™ potrwaÄ‡)...")

SESSIONS_DIR = "sessions"
SEARCH_COLLECTION = "polskie_prawo"

if not os.path.exists(SESSIONS_DIR):
    os.makedirs(SESSIONS_DIR)

@st.cache_resource
def load_resources() -> Tuple:
    """Åaduje zasoby AI: Qdrant, embeddery i model jÄ™zykowy."""
    print("LOG: Importowanie bibliotek...")

    from unsloth import FastLanguageModel
    from qdrant_client import QdrantClient
    from sentence_transformers import SentenceTransformer
    from fastembed import SparseTextEmbedding

    MODEL_ID = "speakleash/Bielik-11B-v2.6-Instruct"
    QDRANT_PATH = "./qdrant_data"
    EMBEDDING_MODEL = "intfloat/multilingual-e5-large"
    SPARSE_MODEL = "Qdrant/bm25"

    client = QdrantClient(path=QDRANT_PATH)

    dense = SentenceTransformer(EMBEDDING_MODEL, device="cuda")
    sparse = SparseTextEmbedding(model_name=SPARSE_MODEL)

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = MODEL_ID,
        max_seq_length = 8192,
        dtype = None,
        load_in_4bit = True,
    )
    FastLanguageModel.for_inference(model)

    return client, dense, sparse, model, tokenizer

try:
    client, dense_embedder, sparse_embedder, model, tokenizer = load_resources()

    loading_placeholder.empty()
    st.toast("System gotowy do pracy!", icon="âœ…")
except Exception as e:
    st.error(f"BÅ‚Ä…d krytyczny podczas Å‚adowania: {e}")
    st.stop()

import torch
from qdrant_client import models


def get_session_file_path(session_id):
    return os.path.join(SESSIONS_DIR, f"{session_id}.json")

def save_current_session():
    """Zapisuje bieÅ¼Ä…cÄ… sesjÄ™ do pliku JSON."""
    if not st.session_state.messages:
        return
    
    if "title" not in st.session_state:
        user_msgs = [m['content'] for m in st.session_state.messages if m['role'] == 'user']
        if user_msgs:
            question = user_msgs[0]
            st.session_state.title = name_session(question)

    data = {
        "id": st.session_state.session_id,
        "title": st.session_state.get("title", "Bez tytuÅ‚u"),
        "timestamp": datetime.now().isoformat(),
        "messages": st.session_state.messages
    }
    
    file_path = get_session_file_path(st.session_state.session_id)
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

def load_session_by_id(session_id):
    """Wczytuje sesjÄ™ z pliku."""
    file_path = get_session_file_path(session_id)
    if os.path.exists(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            st.session_state.session_id = data["id"]
            st.session_state.messages = data["messages"]
            st.session_state.title = data.get("title", "Bez tytuÅ‚u")
    else:
        init_new_session()

def init_new_session():
    """Resetuje stan do nowej, czystej rozmowy."""
    st.session_state.session_id = str(uuid.uuid4())
    st.session_state.messages = []
    if "title" in st.session_state:
        del st.session_state.title

def list_past_sessions():
    """Zwraca listÄ™ dostÄ™pnych plikÃ³w sesji posortowanÄ… od najnowszej."""
    files = glob.glob(os.path.join(SESSIONS_DIR, "*.json"))
    sessions = []
    for f in files:
        try:
            with open(f, "r", encoding="utf-8") as file:
                data = json.load(file)
                sessions.append({
                    "id": data["id"],
                    "title": data.get("title", "Bez tytuÅ‚u"),
                    "path": f,
                    "time": os.path.getmtime(f)
                })
        except:
            continue
    return sorted(sessions, key=lambda x: x["time"], reverse=True)

if "session_id" not in st.session_state:
    init_new_session()

def name_session(question: str) -> str:
    """Nazywa sesjÄ™ na podstawie pierwszego pytania uÅ¼ytkownika."""
    prompt = f"""
    JesteÅ› asystentem, ktÃ³ry tworzy zwiÄ™zÅ‚e tytuÅ‚y dla rozmÃ³w na podstawie pierwszego pytania uÅ¼ytkownika.
    ZASADY:
    1. TytuÅ‚ musi byÄ‡ krÃ³tki (maksymalnie 5 sÅ‚Ã³w).
    2. TytuÅ‚ musi byÄ‡ precyzyjny i odzwierciedlaÄ‡ temat pytania.
    3. Unikaj ogÃ³lnych fraz jak "Rozmowa z AI" czy "Pytanie prawne".
    4. UÅ¼ywaj jÄ™zyka polskiego.
    5. Wypisz tytuÅ‚ w formie, ktÃ³rÄ… mogÄ™ wpisaÄ‡ w Google.
    PIERWSZE PYTANIE: "{question}"
    TYTUÅ ROZMOWY:
    """
    messages = [{"role": "user", "content": prompt}]
    inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs_tensor = tokenizer(inputs, return_tensors="pt").to("cuda")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs_tensor, 
            max_new_tokens=32,
            temperature=0.3,  
            do_sample=True,  
            use_cache=True
        )
    
    title = tokenizer.decode(outputs[0][inputs_tensor.input_ids.shape[1]:], skip_special_tokens=True).strip()
    cleaned_title = title.replace('"', '').strip()

    if len(cleaned_title) < 3: 
        return "Nowa rozmowa"

    return cleaned_title

def rewrite_query(user_query, chat_history) -> str:
    """
    Inteligentnie przepisuje krÃ³tkie pytania na peÅ‚ne 
    zapytania do bazy, wykorzystujÄ…c historiÄ™ rozmowy.
    """
    if not chat_history:
        return user_query
    
    short_history = chat_history[-4:] 
    
    rewrite_prompt = f"""
    JesteÅ› prawnikiem-lingwistÄ…. Twoim zadaniem jest przetÅ‚umaczenie potocznego pytania klienta na profesjonalne zapytanie do wyszukiwarki prawniczej.
    ZASADY:
    1. ZamieÅ„ sÅ‚owa potoczne na ustawowe (np. "morderstwo" -> "zabÃ³jstwo", "ukradÅ‚ auto" -> "zabÃ³r pojazdu mechanicznego").
    2. UwzglÄ™dnij kontekst z historii rozmowy (jeÅ›li jest).
    3. Wynik ma byÄ‡ jednym, precyzyjnym zdaniem pytajÄ…cym.

    HISTORIA: {short_history}
    OSTATNIE PYTANIE: "{user_query}"

    PROFESJONALNE ZAPYTANIE:
    """

    messages = [{"role": "user", "content": rewrite_prompt}]
    inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs_tensor = tokenizer(inputs, return_tensors="pt").to("cuda")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs_tensor, 
            max_new_tokens=128,
            temperature=0.1,  
            do_sample=True,  
            use_cache=True
        )
    
    rewritten = tokenizer.decode(outputs[0][inputs_tensor.input_ids.shape[1]:], skip_special_tokens=True).strip()
    cleaned = rewritten.replace('"', '').replace("PROFESJONALNE ZAPYTANIE:", "").strip()

    if len(cleaned) < 3: 
        return user_query

    return cleaned

def search_law(query: str, top_k: int = 10, score_threshold: float = 0.6) -> List[Dict]:
    """
    Szuka w kaÅ¼dej kolekcji, Å‚Ä…czy wyniki i zwraca X najlepszych globalnie.
    """
    dense_vec = dense_embedder.encode([f"query: {query}"], normalize_embeddings=True)[0].tolist()
    sparse_res = list(sparse_embedder.embed([query]))[0]
    
    qdrant_sparse = models.SparseVector(
        indices=sparse_res.indices.tolist(), 
        values=sparse_res.values.tolist()
    )

    all_hits = []

    if client.collection_exists(SEARCH_COLLECTION):
        hits = client.query_points(
            collection_name=SEARCH_COLLECTION,
            prefetch=[
                models.Prefetch(
                    query=dense_vec,
                    using="dense",
                    limit=30,
                ),
                models.Prefetch(
                    query=qdrant_sparse,
                    using="sparse",
                    limit=30,
                )
            ],
            query=models.FusionQuery(fusion=models.Fusion.RRF),
            limit=top_k
        ).points

        valid_hits = [hit for hit in hits if hit.score > score_threshold]
        all_hits.extend(valid_hits)

    return all_hits[:top_k]

with st.sidebar:
    st.title("Historia")
    
    if st.button("Nowy czat", use_container_width=True, type="secondary"):
        save_current_session()
        init_new_session()
        st.rerun()
    
    st.markdown("---")
    st.caption("Poprzednie rozmowy:")
    
    sessions = list_past_sessions()
    for s in sessions:
        if st.button(s["title"], key=s["id"], use_container_width=True):
            save_current_session()
            load_session_by_id(s["id"]) 
            st.rerun()

    st.markdown("---")
    st.info("Status: Online ğŸŸ¢  \nTryb: Persisted (Dysk)")

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("O co chcesz zapytaÄ‡?"):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    save_current_session()

    context_text = ""

    with st.status("AnalizujÄ™ przepisy...", expanded=True) as status:
        st.write("ğŸ”„ AnalizujÄ™ pytanie...")
        search_query = rewrite_query(prompt, st.session_state.messages[:-1])
        
        st.write("ğŸ” PrzeszukujÄ™ Kodeksy...")
        hits = search_law(search_query, top_k=5)
        
        if hits:
            for hit in hits:
                meta = hit.payload
                source_label = meta.get('source', 'Akt Prawny')
                article_label = meta.get('article', 'Art. ?')
                text_content = meta.get('full_markdown', meta.get('text', ''))
                
                context_text += f"=== {source_label} | {article_label} ===\n{text_content}\n\n"
        else:
            context_text = "Brak przepisÃ³w."
            
        status.update(label="Analiza zakoÅ„czona!", state="complete", expanded=False)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        with st.spinner("PiszÄ™ opiniÄ™ prawnÄ…..."):
            system_prompt = """
            JesteÅ› ekspertem od polskiego prawa. Twoim zadaniem jest interpretacja przepisÃ³w i udzielenie profesjonalnej porady.
            DziaÅ‚asz w oparciu o dostarczony KONTEKST PRAWNY, ktÃ³ry moÅ¼e zawieraÄ‡ rÃ³Å¼ne kodeksy (Karny, Cywilny, Pracy, WykroczeÅ„) oraz KonstytucjÄ™.

            ZASADY:
            1. Hierarchia: Konstytucja > Ustawy (Kodeksy). JeÅ›li problem dotyczy praw podstawowych, zacznij od Konstytucji.
            2. Kontekst: UÅ¼ywaj tylko przepisÃ³w dostarczonych w sekcji KONTEKST.
            3. Precyzja: OdpowiedÅº musi byÄ‡ konkretna. JeÅ›li pytanie dotyczy pracy, skup siÄ™ na Kodeksie Pracy. JeÅ›li przestÄ™pstwa - na Karnym.
            4. Struktura odpowiedzi:
            - Podstawa Prawna (wymieÅ„ artykuÅ‚y i nazwy aktÃ³w)
            - Analiza (interpretacja sytuacji w Å›wietle przepisÃ³w)
            - Konkluzja (jasne wnioski dla klienta)
            5. NajwaÅ¼niejsze - jeÅ›li brak przepisÃ³w w kontekÅ›cie, przyznaj to otwarcie i zasugeruj konsultacjÄ™ z prawnikiem.
            6. Nie wymyÅ›laj przepisÃ³w ani nie odwoÅ‚uj siÄ™ do nieistniejÄ…cych artykuÅ‚Ã³w.
            7. Nie naciÄ…gaj kontekstu - jeÅ›li pytanie wykracza poza dostarczone przepisy, przyznaj to.

            RESTKRYKCYJNE ZASADY FORMATOWANIA (MODEL MUSI ICH PRZESTRZEGAÄ†):
            KaÅ¼da odpowiedÅº musi skÅ‚adaÄ‡ siÄ™ wyÅ‚Ä…cznie z 4 sekcji oznaczonych nagÅ‚Ã³wkami H2 (##). Nie dodawaj Å¼adnego tekstu przed pierwszÄ… sekcjÄ… ani po ostatniej.

            STRUKTURA ODPOWIEDZI:

            ## Podstawa Prawna
            W tej sekcji wymieÅ„ przepisy w formie listy wypunktowanej.
            BEZWZGLÄ˜DNY FORMAT CYTOWANIA:
            * **Art. {numer} Â§ {numer_paragrafu} {PeÅ‚na Nazwa Kodeksu}:** {treÅ›Ä‡ przepisu}

            Zasady dla cytatÃ³w:
            - JeÅ›li przepis nie ma paragrafu, pomiÅ„ znak Â§ i numer paragrafu (np. Art. 148 Kodeksu Karnego:).
            - Zawsze podawaj peÅ‚nÄ… nazwÄ™ kodeksu (np. "Kodeksu Karnego", a nie "k.k.").
            - TreÅ›Ä‡ przepisu musi byÄ‡ przytoczona po dwukropku.

            ## Analiza
            SzczegÃ³Å‚owa interpretacja sytuacji w Å›wietle przytoczonych wyÅ¼ej przepisÃ³w. OdnieÅ› siÄ™ bezpoÅ›rednio do faktÃ³w z zapytania uÅ¼ytkownika. WyjaÅ›nij przesÅ‚anki (np. "uÅ¼ycie przemocy", "stan nietrzeÅºwoÅ›ci"). Pisz akapitami.

            ## Konkluzja
            Jasne i zwiÄ™zÅ‚e wnioski. JeÅ›li wynik zaleÅ¼y od zmiennych (np. czy uÅ¼yto broni), zastosuj listÄ™ wypunktowanÄ…, aby pokazaÄ‡ warianty:
            * Wariant A: konsekwencja.
            * Wariant B: konsekwencja.

            ## Podsumowanie
            Jedno lub dwa zdania streszczenia dla klienta, stanowiÄ…ce "tl;dr" caÅ‚ej porady. Najlepiej by byÅ‚o, gdyby zawieraÅ‚o bezpoÅ›redniÄ…, konkretnÄ… odpowiedÅº na pytanie uÅ¼ytkownika.

            Na koÅ„cu odpowiedzi doÅ‚Ä…cz sekcjÄ™ Å¹rÃ³dÅ‚a, gdzie w jednej linii wymienisz wszystkie cytowane artykuÅ‚y w formacie:
            BEZWZGLÄ˜DNY FORMAT WYPISYWANIA Å¹RÃ“DEÅ:
            "\n\n---\nğŸ“š **Å¹rÃ³dÅ‚a:** Art. {numer} {PeÅ‚na Nazwa Kodeksu}."
            PrzykÅ‚ad:
            "\n\n---\nğŸ“š **Å¹rÃ³dÅ‚a:** Art. 134, 135, 136, 148 Kodeksu Karnego."
            Nie zapisuj tego jako osobny nagÅ‚owek, tylko jako zwykÅ‚y tekst od nowej linii oraz nie wypisuj paragrafÃ³w w ÅºrÃ³dÅ‚ach.
            """
            
            messages_payload = [{"role": "system", "content": system_prompt}]
            for msg in st.session_state.messages[:-1]:
                messages_payload.append(msg)
                
            current_input = f"KONTEKST PRAWNY:\n{context_text}\n\nPYTANIE:\n{prompt}"
            messages_payload.append({"role": "user", "content": current_input})

            model_inputs = tokenizer.apply_chat_template(messages_payload, tokenize=False, add_generation_prompt=True)
            inputs_tensor = tokenizer(model_inputs, return_tensors="pt").to("cuda")

            with torch.no_grad():
                outputs = model.generate(
                    **inputs_tensor,
                    max_new_tokens=2048,
                    temperature=0.2,
                    repetition_penalty=1.05,
                    do_sample=True,
                    use_cache=True,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0][inputs_tensor.input_ids.shape[1]:], skip_special_tokens=True)

        message_placeholder.markdown(response)

    st.session_state.messages.append({"role": "assistant", "content": response})
    save_current_session()
    if len(st.session_state.messages) == 2:
        st.rerun()