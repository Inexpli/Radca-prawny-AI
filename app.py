import os
import json
import streamlit as st
from typing import List, Dict, Tuple


st.set_page_config(
    page_title="Radca Prawny AI",
    page_icon="âš–ï¸",
    layout="wide"
)

st.title("âš–ï¸ Radca Prawny AI")
st.markdown("TwÃ³j prywatny asystent prawny.")

loading_placeholder = st.empty()
loading_placeholder.info("ğŸš€ Inicjalizacja systemu... \n\n ğŸ› ï¸ Åadowanie bibliotek AI (to moÅ¼e chwilÄ™ potrwaÄ‡)...")

HISTORY_FILE = "chat_history.json"

def load_chat_history() -> List[Dict]:
    """Wczytuje historiÄ™ z pliku JSON jeÅ›li istnieje."""
    if os.path.exists(HISTORY_FILE):
        try:
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception:
            return []
    return []

def save_chat_history(messages) -> None:
    """Zapisuje historiÄ™ do pliku JSON."""
    try:
        with open(HISTORY_FILE, "w", encoding="utf-8") as f:
            json.dump(messages, f, ensure_ascii=False, indent=4)
    except Exception as e:
        print(f"BÅ‚Ä…d zapisu historii: {e}")

@st.cache_resource
def load_resources() -> Tuple:
    """Åaduje zasoby AI: Qdrant, embeddery i model jÄ™zykowy."""
    print("LOG: Importowanie bibliotek...")

    from unsloth import FastLanguageModel
    from qdrant_client import QdrantClient
    from sentence_transformers import SentenceTransformer
    from fastembed import SparseTextEmbedding

    MODEL_ID = "speakleash/Bielik-11B-v2.6-Instruct"
    QDRANT_PATH = "./qdrant_data"
    EMBEDDING_MODEL = "intfloat/multilingual-e5-large"
    SPARSE_MODEL = "Qdrant/bm25"

    client = QdrantClient(path=QDRANT_PATH)

    dense = SentenceTransformer(EMBEDDING_MODEL, device="cuda")
    sparse = SparseTextEmbedding(model_name=SPARSE_MODEL)

    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = MODEL_ID,
        max_seq_length = 8192,
        dtype = None,
        load_in_4bit = True,
    )
    FastLanguageModel.for_inference(model)

    return client, dense, sparse, model, tokenizer

try:
    client, dense_embedder, sparse_embedder, model, tokenizer = load_resources()
    
    loading_placeholder.empty()
    st.toast("System gotowy do pracy!", icon="âœ…")
except Exception as e:
    st.error(f"BÅ‚Ä…d krytyczny podczas Å‚adowania: {e}")
    st.stop()

import torch
from qdrant_client import models

SEARCH_COLLECTIONS = ["polskie_prawo"]

def rewrite_query(user_query, chat_history) -> str:
    """
    Inteligentnie przepisuje krÃ³tkie pytania na peÅ‚ne 
    zapytania do bazy, wykorzystujÄ…c historiÄ™ rozmowy.
    """
    if not chat_history:
        return user_query
    
    short_history = chat_history[-4:] 
    
    rewrite_prompt = f"""
    Twoim zadaniem jest przeredagowanie ostatniego pytania uÅ¼ytkownika tak, aby byÅ‚o w peÅ‚ni zrozumiaÅ‚e bez znajomoÅ›ci poprzednich wiadomoÅ›ci.
    Musisz dodaÄ‡ brakujÄ…cy kontekst (np. o czym byÅ‚a mowa wczeÅ›niej).

    HISTORIA ROZMOWY:
    {short_history}

    OSTATNIE KRÃ“TKIE PYTANIE: "{user_query}"

    ZASADA: Nie odpowiadaj na pytanie. Tylko je przepisz na peÅ‚ne zdanie, ktÃ³re mogÄ™ wpisaÄ‡ w Google.

    PEÅNE PYTANIE:
    """

    messages = [{"role": "user", "content": rewrite_prompt}]
    inputs = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs_tensor = tokenizer(inputs, return_tensors="pt").to("cuda")
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs_tensor, 
            max_new_tokens=128,
            temperature=0.2,  
            do_sample=True,  
            use_cache=True
        )
    
    rewritten = tokenizer.decode(outputs[0][inputs_tensor.input_ids.shape[1]:], skip_special_tokens=True).strip()
    cleaned = rewritten.replace('"', '').replace("PEÅNE PYTANIE:", "").strip()

    if len(cleaned) < 3: 
        return user_query

    return cleaned

def search_law(query: str, top_k: int = 5) -> List[Dict]:
    """
    Szuka w kaÅ¼dej kolekcji, Å‚Ä…czy wyniki i zwraca X najlepszych globalnie.
    """
    dense_vec = dense_embedder.encode([f"query: {query}"], normalize_embeddings=True)[0].tolist()
    sparse_res = list(sparse_embedder.embed([query]))[0]
    
    qdrant_sparse = models.SparseVector(
        indices=sparse_res.indices.tolist(), 
        values=sparse_res.values.tolist()
    )

    all_hits = []
    for collection in SEARCH_COLLECTIONS:
        if client.collection_exists(collection):
            hits = client.query_points(
                collection_name=collection,
                prefetch=[
                    models.Prefetch(query=dense_vec, using="dense", limit=20),
                    models.Prefetch(query=qdrant_sparse, using="sparse", limit=20),
                ],
                query=models.FusionQuery(fusion=models.Fusion.RRF),
                limit=top_k
            ).points
            all_hits.extend(hits)
            
    return all_hits[:top_k]

with st.sidebar:
    st.title("âš™ï¸ Ustawienia")
    if st.button("WyczyÅ›Ä‡ historiÄ™"):
        st.session_state.messages = []
        if os.path.exists(HISTORY_FILE):
            os.remove(HISTORY_FILE)
        st.rerun()
        
    st.info("Status: Online ğŸŸ¢\n\nTryb: Persisted (Dysk)")

if "messages" not in st.session_state:
    st.session_state.messages = load_chat_history()

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("O co chcesz zapytaÄ‡?"):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})
    save_chat_history(st.session_state.messages)

    candidates = [] 
    context_text = ""

    with st.status("AnalizujÄ™ przepisy...", expanded=True) as status:
        st.write("ğŸ”„ AnalizujÄ™ pytanie...")
        search_query = rewrite_query(prompt, st.session_state.messages[:-1])
        
        st.write("ğŸ” PrzeszukujÄ™ Kodeksy...")
        hits = search_law(search_query, top_k=5)
        
        if hits:
            for hit in hits:
                meta = hit.payload
                source_label = meta.get('source', 'Akt Prawny')
                article_label = meta.get('article', 'Art. ?')
                text_content = meta.get('full_markdown', meta.get('text', ''))
                
                context_text += f"=== {source_label} | {article_label} ===\n{text_content}\n\n"
                
                candidates.append({
                    "full_label": f"{article_label} ({source_label})",
                    "article_id": article_label
                })
        else:
            context_text = "Brak przepisÃ³w."
            
        status.update(label="Analiza zakoÅ„czona!", state="complete", expanded=False)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        
        with st.spinner("PiszÄ™ opiniÄ™ prawnÄ…..."):
            system_prompt = """JesteÅ› ekspertem od polskiego prawa. Twoim zadaniem jest interpretacja przepisÃ³w i udzielenie profesjonalnej porady.
            DziaÅ‚asz w oparciu o dostarczony KONTEKST PRAWNY, ktÃ³ry moÅ¼e zawieraÄ‡ rÃ³Å¼ne kodeksy (Karny, Cywilny, Pracy, WykroczeÅ„) oraz KonstytucjÄ™.

            ZASADY:
            1. Hierarchia: Konstytucja > Ustawy (Kodeksy). JeÅ›li problem dotyczy praw podstawowych, zacznij od Konstytucji.
            2. Kontekst: UÅ¼ywaj tylko przepisÃ³w dostarczonych w sekcji KONTEKST.
            3. Precyzja: OdpowiedÅº musi byÄ‡ konkretna. JeÅ›li pytanie dotyczy pracy, skup siÄ™ na Kodeksie Pracy. JeÅ›li przestÄ™pstwa - na Karnym.
            4. Struktura odpowiedzi:
            - Podstawa Prawna (wymieÅ„ artykuÅ‚y i nazwy aktÃ³w)
            - Analiza (interpretacja sytuacji w Å›wietle przepisÃ³w)
            - Konkluzja (jasne wnioski dla klienta)

            RESTKRYKCYJNE ZASADY FORMATOWANIA (MODEL MUSI ICH PRZESTRZEGAÄ†):
            KaÅ¼da odpowiedÅº musi skÅ‚adaÄ‡ siÄ™ wyÅ‚Ä…cznie z 4 sekcji oznaczonych nagÅ‚Ã³wkami H2 (##). Nie dodawaj Å¼adnego tekstu przed pierwszÄ… sekcjÄ… ani po ostatniej.

            STRUKTURA ODPOWIEDZI:

            ## Podstawa Prawna
            W tej sekcji wymieÅ„ przepisy w formie listy wypunktowanej.
            BEZWZGLÄ˜DNY FORMAT CYTOWANIA:
            * **Art. {numer} Â§ {numer_paragrafu} {PeÅ‚na Nazwa Kodeksu}:** {treÅ›Ä‡ przepisu}

            Zasady dla cytatÃ³w:
            - JeÅ›li przepis nie ma paragrafu, pomiÅ„ znak Â§ i numer paragrafu (np. Art. 148 Kodeksu Karnego:).
            - Zawsze podawaj peÅ‚nÄ… nazwÄ™ kodeksu (np. "Kodeksu Karnego", a nie "k.k.").
            - TreÅ›Ä‡ przepisu musi byÄ‡ przytoczona po dwukropku.

            ## Analiza
            SzczegÃ³Å‚owa interpretacja sytuacji w Å›wietle przytoczonych wyÅ¼ej przepisÃ³w. OdnieÅ› siÄ™ bezpoÅ›rednio do faktÃ³w z zapytania uÅ¼ytkownika. WyjaÅ›nij przesÅ‚anki (np. "uÅ¼ycie przemocy", "stan nietrzeÅºwoÅ›ci"). Pisz akapitami.

            ## Konkluzja
            Jasne i zwiÄ™zÅ‚e wnioski. JeÅ›li wynik zaleÅ¼y od zmiennych (np. czy uÅ¼yto broni), zastosuj listÄ™ wypunktowanÄ…, aby pokazaÄ‡ warianty:
            * Wariant A: konsekwencja.
            * Wariant B: konsekwencja.

            ## Podsumowanie
            Jedno lub dwa zdania streszczenia dla klienta, stanowiÄ…ce "tl;dr" caÅ‚ej porady. Najlepiej by byÅ‚o, gdyby zawieraÅ‚o bezpoÅ›redniÄ…, konkretnÄ… odpowiedÅº na pytanie uÅ¼ytkownika.

            PamiÄ™taj: Twoim priorytetem jest poprawnoÅ›Ä‡ merytoryczna oraz Å›cisÅ‚e trzymanie siÄ™ formatu "Art. ... Â§ ... Kodeksu ...:".
            """
            
            messages_payload = [{"role": "system", "content": system_prompt}]
            for msg in st.session_state.messages[:-1]:
                messages_payload.append(msg)
                
            current_input = f"KONTEKST PRAWNY:\n{context_text}\n\nPYTANIE:\n{prompt}"
            messages_payload.append({"role": "user", "content": current_input})

            model_inputs = tokenizer.apply_chat_template(messages_payload, tokenize=False, add_generation_prompt=True)
            inputs_tensor = tokenizer(model_inputs, return_tensors="pt").to("cuda")

            with torch.no_grad():
                outputs = model.generate(
                    **inputs_tensor,
                    max_new_tokens=2048,
                    temperature=0.2,
                    repetition_penalty=1.05,
                    do_sample=True,
                    eos_token_id=tokenizer.eos_token_id
                )
            
            response = tokenizer.decode(outputs[0][inputs_tensor.input_ids.shape[1]:], skip_special_tokens=True)
    
            final_sources = []
            for candidate in candidates:
                if candidate["article_id"] in response:
                    final_sources.append(candidate["full_label"])

            if final_sources:
                footer = "\n\n---\nğŸ“š **Å¹rÃ³dÅ‚a:** " + ", ".join(final_sources)
                final_response = response + footer
            else:
                final_response = response

        message_placeholder.markdown(final_response)

    st.session_state.messages.append({"role": "assistant", "content": final_response})
    save_chat_history(st.session_state.messages)